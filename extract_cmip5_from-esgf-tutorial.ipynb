{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to extract CMIP5 data for the North-West European Shelf\n",
    "\n",
    "This notebook shows how to extract any CMIP5 data for the north-west European Shelf. The example of monthly SST from the HadGEM2-ES model will be used, but it can be easily adapted to deal with any model and any variable. \n",
    "\n",
    "\n",
    "You will likely need to install a few packages to get this working. So just run the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    conda install nctoolkit -y\n",
    "    pip install git+https://github.com/r4ecology/cmipsearch.git\n",
    "    pip install pyesg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cmipsearch\n",
    "import random\n",
    "import nctoolkit as nc\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "nc.options(lazy = True)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log on to ESGF\n",
    "\n",
    "Some of the files require you to be logged on to access them. If you do not have an id, get one here: https://esgf-node.llnl.gov/search/cmip5/.\n",
    "\n",
    "The data available in this tutorial does not require that you log in, so ignore this if you do not want to register."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyesgf.logon import LogonManager\n",
    "lm = LogonManager()\n",
    "lm.logoff()\n",
    "lm.is_logged_on()\n",
    "lm.logon(hostname='esgf-node.llnl.gov', interactive=True, bootstrap=True)\n",
    "lm.is_logged_on()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMIP5 data is available from multiple nodes, so if you want to get all available data, you need to search all nodes. The small package cmipsearch available on the PML modelling group's GitHub page will do just this. For this tutorial, we will extract HadGEM2-ES SST data for the historical time period, which is before 2006, and RCP 8.5 which is 2006 onwards. (Or not quite, because the Hadley Centre's historical scenario mysteriously ends in November 2005....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = []\n",
    "for rcp in [\"historical\",  \"rcp85\"]:\n",
    "    ensemble.append(cmipsearch.cmip5_search(frequency=\"mon\", var = \"tos\", experiment=[rcp], models = \"HadGEM2-ES\"))\n",
    "ensemble = pd.concat(ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then restrict the files to the years from 1985 to 2100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.query(\"start < 2101\").query(\"end > 1985\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are available via OPeNDAP. This means that you do not need to download global data. Instead you can select a sub-region that will include the north-west European shelf. We can do this using NCO. Note: CDO is not well suited to this as it can only extract the entire planet.\n",
    "\n",
    "But first, we need a way to generate the NCO call. We just need to derive the call once for a model we are dealing with. The function below will do this for the NW shelf. In essence, it will extract the global data for the first time step, and then use that to figure out the NCO call that will give you the NW shelf. It's a bit hacked together, but it does the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the global data for the 1st time step\n",
    "def generate_call(ff, lon_range, lat_range):\n",
    "    if \"MIROC4h\" in ff:\n",
    "        return \"ncks -d rlon,280.,400. -d rlat,40.,80.\"\n",
    "    data = nc.open_thredds(ff)\n",
    "    data.select(timesteps=0)\n",
    "    data.run()\n",
    "    # Work out the name of longitude and latitude\n",
    "    if len([x for x in list(data.to_xarray().dims) if \"lon\" in x]) == 1:\n",
    "        lon_name = [x for x in list(data.to_xarray().dims) if \"lon\" in x ][0]\n",
    "    else:\n",
    "        lon_name = None\n",
    "    if len([x for x in list(data.to_xarray().dims) if \"lat\" in x]) == 1:\n",
    "        lat_name = [x for x in list(data.to_xarray().dims) if \"lat\" in x ][0]\n",
    "    else:\n",
    "        lat_name = None\n",
    "    \n",
    "    ds= data.to_xarray() \n",
    "    lon_coord = [x for x in ds.coords if \"lon\" == x ][0]\n",
    "    if (lon_name is not None) and (len(ds[lon_coord].values.shape) == 1):\n",
    "        if len(np.shape(ds[lon_name].values)) == 1:\n",
    "            dimensions = 1\n",
    "        else:\n",
    "            dimensions = 2\n",
    "            \n",
    "        units = ds[lon_name].attrs[\"units\"]\n",
    "        \n",
    "        # 1D case\n",
    "        \n",
    "        if dimensions == 1:\n",
    "            # if units are in degrees east, we need to use wrapping coordinates\n",
    "            if units == \"degrees_east\":\n",
    "                if lon_range[0] < 0:\n",
    "                    lon_min = 360 + lon_range[0]\n",
    "                else:\n",
    "                    lon_min = lon_range[0]\n",
    "                    \n",
    "                if lon_range[1] < 0:\n",
    "                    lon_max = 360 + lon_range[1]\n",
    "                else:\n",
    "                    lon_max = lon_range[1]\n",
    "                    \n",
    "                lat_min = lat_range[0]\n",
    "                lat_max = lat_range[1]\n",
    "            else:\n",
    "                lon_min = lon_range[0]\n",
    "                lon_max = lon_range[1]\n",
    "                lat_min = lat_range[0]\n",
    "                lat_max = lat_range[1]\n",
    "            \n",
    "            if lon_name == \"rlon\":\n",
    "                if min(ds[lon_name].values) > 0:\n",
    "                    lon_min = 310 \n",
    "                    lon_max = 360\n",
    "                    lat_min = 30\n",
    "                    lat_max = 80\n",
    "                else:\n",
    "                    lon_min = lon_range[0]\n",
    "                    lon_max = lon_range[1]\n",
    "            \n",
    "            lon_min = float(lon_min)\n",
    "            lon_max = float(lon_max)\n",
    "            lat_min = float(lat_min)\n",
    "            lat_max = float(lat_max)\n",
    "                \n",
    "                \n",
    "            # If it's rotated, we need this:\n",
    "            return f\"ncks -d {lon_name},{lon_min},{lon_max} -d {lat_name},{lat_min},{lat_max}\"\n",
    "        \n",
    "    ds= data.to_xarray() \n",
    "    units = ds[\"lon\"].attrs[\"units\"]\n",
    "    if \"east\" in units:\n",
    "        \n",
    "        ds = data.to_xarray()\n",
    "        lon_coord = [x for x in data.to_xarray().coords if \"lon\" in x ][0]\n",
    "        if len(ds[lon_coord].values.shape) > 1:\n",
    "            lon_coord\n",
    "            lat_coord = [x for x in data.to_xarray().coords if \"lat\" in x ][0]\n",
    "            list(ds[lat_coord].dims)\n",
    "            lon_min = lon_range \n",
    "            lat_min = lat_range \n",
    "            df1 = (\n",
    "            ds\n",
    "                .coords[lon_coord]\n",
    "                .to_dataframe()\n",
    "                .loc[:,[lon_coord]]\n",
    "                .iloc[:,1:]\n",
    "                .reset_index()\n",
    "            #     .query(\"lon  > 13\")\n",
    "                .query(f\"{lon_coord} < {lon_min[1]} | {lon_coord} > {lon_min[0] + 360}\")\n",
    "                .drop(columns = [lon_coord])\n",
    "                .reset_index(drop = True)\n",
    "            )\n",
    "            df2 = (\n",
    "            ds\n",
    "                .coords[lat_coord]\n",
    "                .to_dataframe()\n",
    "                .loc[:,[lat_coord]]\n",
    "                .iloc[:,1:]\n",
    "                .reset_index()\n",
    "            #     .query(\"lon  > 13\")\n",
    "                .query(f\"{lat_coord} > {lat_min[0]}\")\n",
    "                .query(f\"{lat_coord} < {lat_min[1]}\")\n",
    "                .drop(columns = [lat_coord])\n",
    "                .reset_index(drop = True)\n",
    "            )\n",
    "            merged = df1.merge(df2) \n",
    "            \n",
    "            command = f\"ncks \"\n",
    "            \n",
    "            for cc in merged.columns:\n",
    "                cc_min = min(merged.loc[:,[cc]].values)[0]\n",
    "                cc_max = max(merged.loc[:,[cc]].values)[0]\n",
    "                command = f\"{command} -d {cc},{cc_min},{cc_max}\"\n",
    "            command\n",
    "    return command\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 10 files for HadGEM2-ES. But these cover multiple variants, i.e. starting conditions. Let's work with variant r1i1p1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.query(\"variant == 'r1i1p1'\").reset_index(drop = True)\n",
    "ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now cmipsearch only provides urls for downloads, not the OpenDAP urls. But this can be fixed easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble[\"url\"] = [ff.replace(\"fileServer\",\"dodsC\") for ff in ensemble.url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us 3 files. One covers the historical period. The others cover the RCP period. \n",
    "\n",
    "We now need to generate the call to extract cropped versions of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nco_call = generate_call(ensemble.url[0], lon_range = [-25, 25], lat_range = [35, 75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call is as follows. NCO will crop longitudes between 335 and 25. Essentially 335 is 25 degrees west. Longitude is \"degrees east\" in HadGEM2-ES, and NCO is not smart enough to figure this out itself, so you need to use wrapped coordinates. The latitudes in the call are 35 and 75 as you would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nco_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use nctoolkit to put all of these files into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = nc.open_thredds(ensemble[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to call NCO to extract the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst.nco_command(nco_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, this can be done in parallel. I need to work out how sensible that is, as parallel processing with OpenDAP servers can be volatile.\n",
    "\n",
    "We now want to merge the files. Note that we first need to crop it using CDO. This will convert the file into a more sensible format, to tidy up NCO's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst.crop(lon_range = [-25, 25], lat_range = [35, 75])\n",
    "sst.merge_time()\n",
    "sst.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset with projected SST changes under RCP 8.5. And we can then do things like plot the spatial averaged annual SST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst.spatial_mean()\n",
    "sst.annual_mean()\n",
    "sst.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
